{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordEmbeddingAbbDisambiguation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOOKZjTCvct23iWMzRDiqmg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH27S4bS2pk8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3LD_Oze3AHf"
      },
      "source": [
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import csv\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "from sklearn.metrics import accuracy_score, average_precision_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import  confusion_matrix\n",
        "np.random.seed(500)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stoplist = stopwords.words('english')\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "ps =PorterStemmer()\n",
        "\n",
        "column_names = [\"Abb\", \"sense\" ,\"ABB_frm\" ,\"start_pos\",\"end_pos\", \"info\",\"context\"]\n",
        "txt_path='/content/drive/My Drive/Colab Notebooks/AnonymizedClinicalAbbreviationsAndAcronymsDataSet.txt'\n",
        "data_pd=pd.read_csv( txt_path, sep=\"|\", header=None,encoding='cp1252',)\n",
        "\n",
        "print(data_pd.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NekGpFeK3PaF"
      },
      "source": [
        "sub_pd=data_pd.loc[data_pd[0]=='SS']\n",
        "#sub_pd.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKcVCzzLmqc1"
      },
      "source": [
        "# detect annotation error\n",
        "\n",
        "for index,row in sub_pd.iterrows():\n",
        "    doc=nlp(str(row[6]))\n",
        "    for k,s in enumerate(doc.sents):\n",
        "        \n",
        "        for j,token in enumerate(s):\n",
        "            \n",
        "            if(token.idx==row[3] and token.text != row[0]):\n",
        "                \n",
        "                print(index, token.idx,token.text,s[j+1],s[j+1].idx)\n",
        "                \n",
        "                # modify \n",
        "                sub_pd.at[index,3]=s[j+1].idx\n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVBU6KpenPO-"
      },
      "source": [
        "i=[34024,34185,34496,34499]\n",
        "for j in i:\n",
        "  sub_pd=sub_pd.drop(j)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb9-G6jOmqyH"
      },
      "source": [
        "#sub_18_all=pd.DataFrame(sub_pd)\n",
        "sub_18_all=sub_18_all.append(sub_pd)\n",
        "sub_18_all.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNoAM8PzsjJk"
      },
      "source": [
        "sub_18_all.to_csv(r'sub_18_all.txt', header=None, index=None, sep='|', mode='a')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKZWKvTC3mfr"
      },
      "source": [
        "embedding_matrix = np.zeros((1, 50))\n",
        "for index, row in sub_pd.iterrows():\n",
        "  get_sum_embedding(row[6],row[3])\n",
        "  embedding_matrix = np.vstack([embedding_matrix, get_sum_embedding(row[6],row[3])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr--dtFhuV32"
      },
      "source": [
        "embedding_matrix.shape\n",
        "x = np.delete(embedding_matrix, (0), axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz_ZFsCFvDHk"
      },
      "source": [
        "sense= []\n",
        "for index, row in sub_pd.iterrows():\n",
        "  sense.append(str(row[1]))\n",
        "len(sense)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_201HuevwaX"
      },
      "source": [
        "X_train, X_test, Train_Y, Test_Y= train_test_data(x,sense)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPCKH8UVv86Z"
      },
      "source": [
        "def train_test_data(feature_list,sense):\n",
        "  v = DictVectorizer(sparse=None)\n",
        "  X=feature_list\n",
        "  #X = v.fit_transform(feature_list)\n",
        "  Encoder = LabelEncoder()\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, sense, test_size = 0.2, random_state=0)\n",
        "  Train_Y = Encoder.fit_transform(y_train)\n",
        "  Test_Y = Encoder.fit_transform(y_test)\n",
        "  #print(len(X_train ),len(y_train))\n",
        "  return X_train, X_test, Train_Y, Test_Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PKEVZxAwKZf"
      },
      "source": [
        "print(SVM_class(X_train, X_test, Train_Y, Test_Y))\n",
        "print(G_NB(X_train,Train_Y,X_test,Test_Y))\n",
        "print(KN_class(X_train,Train_Y,X_test,Test_Y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hidL7BRwPfe"
      },
      "source": [
        "def SVM_class(X_train, X_test, Train_Y, Test_Y):\n",
        "  SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "  SVM.fit(X_train,Train_Y)\n",
        "  # predict the labels on validation dataset\n",
        "  predictions_SVM = SVM.predict(X_test)\n",
        "  # Use accuracy_score function to get the accuracy\n",
        "  #print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
        "  #print(average_precision_score(predictions_SVM, Test_Y))\n",
        "  #print (classification_report(Test_Y, predictions_SVM))\n",
        "  return str(accuracy_score(predictions_SVM, Test_Y)*100)\n",
        "\n",
        "def G_NB(X_train,Train_Y,X_test,Test_Y):\n",
        "  #Create a Gaussian Classifier\n",
        "   model = GaussianNB()\n",
        "\n",
        "   # Train the model using the training sets\n",
        "   model.fit(X_train,Train_Y)\n",
        "\n",
        "   #Predict Output\n",
        "   predicted= model.predict(X_test) \n",
        "   #print(\"Accuracy:\",metrics.accuracy_score(Test_Y, predicted))\n",
        "   return str(metrics.accuracy_score(Test_Y, predicted))\n",
        "\n",
        "def KN_class(X_train,Train_Y,X_test,Test_Y):\n",
        "  classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "  classifier.fit(X_train,Train_Y)\n",
        "  y_pred = classifier.predict(X_test)\n",
        "  #print(\"Accuracy:\",metrics.accuracy_score(Test_Y, y_pred))\n",
        "  return str(metrics.accuracy_score(Test_Y, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfbCDS0e4kXm"
      },
      "source": [
        "def get_sum_embedding(sent,pos):\n",
        "  doc=nlp(sent)\n",
        "  list_str=[]\n",
        "  vv = np.zeros((1,50))\n",
        "  for k,s in enumerate(doc.sents):\n",
        "    for j,token in enumerate(s):\n",
        "      if (token.idx==pos):\n",
        "        print(s,len(s),token.idx, token.text)\n",
        "        for i in range(0,len(s)):\n",
        "          if str(s[i])==token.text or check_special(str(s[i]))==False:\n",
        "            continue\n",
        "          else:\n",
        "            #get embedding\n",
        "            vv=vv+get_embedding('/content/drive/My Drive/Colab Notebooks/glove.6B.50d.txt',str(s[i]).lower(), 50)\n",
        "            #print(s[i],get_embedding('/content/drive/My Drive/Colab Notebooks/glove.6B.50d.txt',str(s[i]).lower(), 50))\n",
        "  return vv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo9x2JVV8pi5"
      },
      "source": [
        "#Get Embedding Vector\n",
        "def get_embedding(filepath, word_, embedding_dim):\n",
        "  #vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "  v = np.zeros((1,50))\n",
        "  with open(filepath) as f:\n",
        "    for line in f:\n",
        "      word, *vector = line.split()\n",
        "      if word ==word_:\n",
        "         v = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "  return v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoJBGVSVFPiR"
      },
      "source": [
        "#ignore special sympols\n",
        "def check_special(wordSpecial):\n",
        "  t= False\n",
        "  if(bool(re.search('^[a-zA-Z0-9]*$',wordSpecial))==True):\n",
        "    t=True\n",
        "  return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNCiKNJYgAA0"
      },
      "source": [
        "#customize sentence boundaries\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "print(\"Before:\", [sent.text for sent in doc.sents])\n",
        "\n",
        "def set_custom_boundaries(doc):\n",
        "    for token in doc[:-1]:\n",
        "        if token.text=='%' or token.text.endswith('%'):\n",
        "            doc[token.i+1].is_sent_start = False\n",
        "    return doc\n",
        "\n",
        "nlp.add_pipe(set_custom_boundaries, before=\"parser\")\n",
        "doc = nlp(text)\n",
        "print(\"After:\", [sent.text for sent in doc.sents])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b87p1mC-dD_G"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwPN6DEurKua"
      },
      "source": [
        "#under developing to check sentence boundaries\n",
        "for token in doc:\n",
        " print(token.is_sent_start, ' '+token.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeOdoRn4Ls3N"
      },
      "source": [
        "sense= []\n",
        "embedding_matrix = np.zeros((1, 50))\n",
        "\n",
        "for index,row in sub_pd.iterrows():\n",
        "  L_Side,R_Side=get_sides(str(row[6]))\n",
        "  embedding_matrix = np.vstack([embedding_matrix, get_embedding_feature(L_Side,R_Side)])\n",
        "\n",
        "x = np.delete(embedding_matrix, (0), axis=0)\n",
        "\n",
        "for index, row in sub_pd.iterrows():\n",
        "  sense.append(str(row[1]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2gOPSjy14Sm"
      },
      "source": [
        "#prepare data for the model and train the model\n",
        "\n",
        "X_train, X_test, Train_Y, Test_Y= train_test_data(x,sense)\n",
        "\n",
        "print(SVM_class(X_train, X_test, Train_Y, Test_Y))\n",
        "print(G_NB(X_train,Train_Y,X_test,Test_Y))\n",
        "print(KN_class(X_train,Train_Y,X_test,Test_Y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLcS8wserwc-"
      },
      "source": [
        "#get left and right side\n",
        "def get_sides(sent):\n",
        "  doc=nlp(sent)\n",
        "  for k,s in enumerate(doc.sents):\n",
        "    for j,token in enumerate(s):\n",
        "      if (token.idx==132):\n",
        "        if j !=0:\n",
        "          L_Side=s[0:j-1]\n",
        "        else:\n",
        "          L_Side=[]\n",
        "        if j != len(s):\n",
        "          R_Side=s[j+1:len(s)-1]\n",
        "        else:\n",
        "          R_Side=[]\n",
        "  return L_Side,R_Side"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8eeuIbeY0dX"
      },
      "source": [
        "#ignore special sympols\n",
        "def check_special(wordSpecial):\n",
        "  t= False\n",
        "  if (bool(re.search('^[a-zA-Z0-9]*$',wordSpecial))==True) and wordSpecial.isdigit()==False:\n",
        "      t=True\n",
        "  return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9kteK-rk30c"
      },
      "source": [
        "#get_feature\n",
        "def get_embedding_feature(l,r):\n",
        "  l,r=get_sides(text)\n",
        "  window_size=r_window_size=0\n",
        "  for i in range(len(l) ,0 , -1):\n",
        "    if check_special(str(l[i])) and window_size < 5:\n",
        "      l_em=get_embedding('/content/drive/My Drive/Colab Notebooks/glove.6B.50d.txt', str(l[i]).lower(), 50)\n",
        "      window_size=window_size+1\n",
        "\n",
        "  for j in range(0,len(r) ):\n",
        "    if check_special(str(r[j])) and r_window_size < 5: \n",
        "      r_em=get_embedding('/content/drive/My Drive/Colab Notebooks/glove.6B.50d.txt', str(r[j]).lower(), 50)\n",
        "      r_window_size = r_window_size+1\n",
        "\n",
        "  total_em=l_em+r_em/(window_size+r_window_size)\n",
        "  return total_em\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwiq8hqvRQPC"
      },
      "source": [
        "CORRECT  some annotations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaRHdtTGRn1R"
      },
      "source": [
        "#customize sentence boundaries\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "#doc = nlp(text)\n",
        "#print(\"Before:\", [sent.text for sent in doc.sents])\n",
        "\n",
        "def set_custom_boundaries(doc):\n",
        "    for token in doc[:-1]:\n",
        "        if token.text=='%' or token.text.endswith('%'):\n",
        "            doc[token.i+1].is_sent_start = False\n",
        "    return doc\n",
        "\n",
        "nlp.add_pipe(set_custom_boundaries, before=\"parser\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytFLikZzRPSW"
      },
      "source": [
        "import pandas as pd\n",
        "txt_path='/content/drive/My Drive/Colab Notebooks/AnonymizedClinicalAbbreviationsAndAcronymsDataSet.txt'\n",
        "data_pd=pd.read_csv( txt_path, sep=\"|\", header=None,encoding='cp1252',)\n",
        "# detect annotation error\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTx-j_YiV1R5"
      },
      "source": [
        "#BAL', 'BK', 'C3', 'CEA', 'CVA', 'CVP', 'CVS', 'DIP', 'ER', 'FISH','NAD', 'OTC', 'PE', 'PT', 'SBP', 'SS', 'ASA', 'AMA'\n",
        "#sub_pd=data_pd.loc[data_pd[0]=='ASA']\n",
        "sub_pd.head()\n",
        "\n",
        "for index,row in sub_pd.iterrows():\n",
        "    doc=nlp(str(row[6]))\n",
        "    for k,s in enumerate(doc.sents):\n",
        "        \n",
        "        for j,token in enumerate(s):\n",
        "            \n",
        "            if(token.idx==row[3] and token.text != row[0]):\n",
        "                \n",
        "                print(index, token.idx,token.text,s[j+1],s[j+1].idx)\n",
        "                \n",
        "                # modify \n",
        "                sub_pd.at[index,3]=s[j+1].idx\n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPcicFzf4hI1"
      },
      "source": [
        "#sub_18_abb=pd.DataFrame()\n",
        "sub_18_abb=sub_18_abb.append(sub_pd)\n",
        "print(sub_18_abb.shape)\n",
        "print(sub_18_abb[0].unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8uU_c8KWDNu"
      },
      "source": [
        "sub_pd=sub_pd.drop(index=2916)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49xNFT7vVDDa"
      },
      "source": [
        "for index,row in sub_pd.iterrows():\n",
        "    doc=nlp(str(row[6]))\n",
        "    for k,s in enumerate(doc.sents):\n",
        "        \n",
        "        for j,token in enumerate(s):\n",
        "            \n",
        "            if(token.idx==row[3] and token.text != row[0]):\n",
        "                \n",
        "                print(index, token.idx,token.text,s[j+1],s[j+1].idx)\n",
        "                \n",
        "                # modify \n",
        "                #sub_pd.at[index,3]=s[j+1].idx\n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxi8BjmRVNQi"
      },
      "source": [
        "sub_18_abb=pd.DataFrame()\n",
        "sub_18_abb=sub_18_abb.append(sub_pd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ_t3DDKViXC"
      },
      "source": [
        "sub_18_abb=sub_18_abb.append(sub_pd)\n",
        "sub_18_abb.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA3NLSoTcfVN"
      },
      "source": [
        "sub_18_abb[0].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzmuvQwMglR3"
      },
      "source": [
        "sub_18_abb.to_csv('sub_18_abb_modified.tsv',sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzEALgHeuX0l"
      },
      "source": [
        ""
      ]
    }
  ]
}