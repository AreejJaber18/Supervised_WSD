{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordEmbeddingAbbDisambiguation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO1mB7SN4jl6dOd8wRW20tC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AreejJaber18/Supervised_WSD/blob/main/WordEmbeddingAbbDisambiguation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH27S4bS2pk8",
        "outputId": "917289f4-1b9d-4131-dc73-1146b62aabe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3LD_Oze3AHf",
        "outputId": "d73c5bc0-4fd5-4c75-85c4-dcc47782b697",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import csv\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "from sklearn.metrics import accuracy_score, average_precision_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import  confusion_matrix\n",
        "np.random.seed(500)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stoplist = stopwords.words('english')\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "ps =PorterStemmer()\n",
        "\n",
        "column_names = [\"Abb\", \"sense\" ,\"ABB_frm\" ,\"start_pos\",\"end_pos\", \"info\",\"context\"]\n",
        "txt_path='/content/drive/My Drive/Colab Notebooks/AnonymizedClinicalAbbreviationsAndAcronymsDataSet.txt'\n",
        "data_pd=pd.read_csv( txt_path, sep=\"|\", header=None,encoding='cp1252',)\n",
        "\n",
        "print(data_pd.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "(37500, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NekGpFeK3PaF"
      },
      "source": [
        "sub_pd=data_pd.loc[data_pd[0]=='SS']\n",
        "#sub_pd.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKcVCzzLmqc1",
        "outputId": "30008654-fe4a-48ad-f4ee-2178371dc1ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# detect annotation error\n",
        "\n",
        "for index,row in sub_pd.iterrows():\n",
        "    doc=nlp(str(row[6]))\n",
        "    for k,s in enumerate(doc.sents):\n",
        "        \n",
        "        for j,token in enumerate(s):\n",
        "            \n",
        "            if(token.idx==row[3] and token.text != row[0]):\n",
        "                \n",
        "                print(index, token.idx,token.text,s[j+1],s[j+1].idx)\n",
        "                \n",
        "                # modify \n",
        "                sub_pd.at[index,3]=s[j+1].idx\n",
        "                "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "34069 179 ( SS 180\n",
            "34167 36 ( SS 37\n",
            "34441 241 ( SS 242\n",
            "34442 171 ( SS 172\n",
            "34443 208 ( SS 209\n",
            "34444 155 ( SS 156\n",
            "34445 206 ( SS 207\n",
            "34446 160 ( SS 161\n",
            "34447 241 ( SS 242\n",
            "34448 237 ( SS 238\n",
            "34449 272 ( SS 273\n",
            "34450 206 ( SS 207\n",
            "34451 263 ( SS 264\n",
            "34452 223 ( SS 224\n",
            "34453 154 ( SS 155\n",
            "34454 169 ( SS 170\n",
            "34455 354 ( SS 355\n",
            "34456 241 ( SS 242\n",
            "34457 224 ( SS 225\n",
            "34458 185 ( SS 186\n",
            "34459 204 ( SS 205\n",
            "34460 229 ( SS 230\n",
            "34461 258 ( SS 259\n",
            "34462 136 ( SS 137\n",
            "34463 316 ( SS 317\n",
            "34464 179 ( SS 180\n",
            "34465 162 ( SS 163\n",
            "34466 545 ( SS 546\n",
            "34467 183 ( SS 184\n",
            "34468 147 ( SS 148\n",
            "34469 339 ( SS 340\n",
            "34470 243 ( SS 244\n",
            "34471 190 ( SS 191\n",
            "34472 332 ( SS 333\n",
            "34473 160 ( SS 161\n",
            "34474 158 ( SS 159\n",
            "34475 167 ( SS 168\n",
            "34477 204 ( SS 205\n",
            "34478 178 ( SS 179\n",
            "34479 258 ( SS 259\n",
            "34480 244 ( SS 245\n",
            "34481 200 ( SS 201\n",
            "34482 512 ( SS 513\n",
            "34483 196 ( SS 197\n",
            "34484 281 ( SS 282\n",
            "34485 286 ( SS 287\n",
            "34486 228 ( SS 229\n",
            "34487 278 ( SS 279\n",
            "34488 409 ( SS 410\n",
            "34489 514 ( SS 515\n",
            "34490 186 ( SS 187\n",
            "34491 195 ( SS 196\n",
            "34492 315 ( SS 316\n",
            "34493 173 ( SS 174\n",
            "34494 399 ( SS 400\n",
            "34495 312 ( SS 313\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVBU6KpenPO-"
      },
      "source": [
        "i=[34024,34185,34496,34499]\n",
        "for j in i:\n",
        "  sub_pd=sub_pd.drop(j)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb9-G6jOmqyH",
        "outputId": "bc900a2f-a6df-4c6d-c728-5231c6b48814",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#sub_18_all=pd.DataFrame(sub_pd)\n",
        "sub_18_all=sub_18_all.append(sub_pd)\n",
        "sub_18_all.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8972, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNoAM8PzsjJk"
      },
      "source": [
        "sub_18_all.to_csv(r'sub_18_all.txt', header=None, index=None, sep='|', mode='a')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKZWKvTC3mfr"
      },
      "source": [
        "embedding_matrix = np.zeros((1, 50))\n",
        "for index, row in sub_pd.iterrows():\n",
        "  get_sum_embedding(row[6],row[3])\n",
        "  embedding_matrix = np.vstack([embedding_matrix, get_sum_embedding(row[6],row[3])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr--dtFhuV32"
      },
      "source": [
        "embedding_matrix.shape\n",
        "x = np.delete(embedding_matrix, (0), axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz_ZFsCFvDHk",
        "outputId": "c61eff32-589a-41fa-b6de-4a45ff7616e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sense= []\n",
        "for index, row in sub_pd.iterrows():\n",
        "  sense.append(str(row[1]))\n",
        "len(sense)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_201HuevwaX"
      },
      "source": [
        "X_train, X_test, Train_Y, Test_Y= train_test_data(x,sense)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPCKH8UVv86Z"
      },
      "source": [
        "def train_test_data(feature_list,sense):\n",
        "  v = DictVectorizer(sparse=None)\n",
        "  X=feature_list\n",
        "  #X = v.fit_transform(feature_list)\n",
        "  Encoder = LabelEncoder()\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, sense, test_size = 0.2, random_state=0)\n",
        "  Train_Y = Encoder.fit_transform(y_train)\n",
        "  Test_Y = Encoder.fit_transform(y_test)\n",
        "  #print(len(X_train ),len(y_train))\n",
        "  return X_train, X_test, Train_Y, Test_Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PKEVZxAwKZf",
        "outputId": "e2ff00e8-8b0c-4892-864a-91c0ef850b80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(SVM_class(X_train, X_test, Train_Y, Test_Y))\n",
        "print(G_NB(X_train,Train_Y,X_test,Test_Y))\n",
        "print(KN_class(X_train,Train_Y,X_test,Test_Y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "96.0\n",
            "0.75\n",
            "0.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hidL7BRwPfe"
      },
      "source": [
        "def SVM_class(X_train, X_test, Train_Y, Test_Y):\n",
        "  SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "  SVM.fit(X_train,Train_Y)\n",
        "  # predict the labels on validation dataset\n",
        "  predictions_SVM = SVM.predict(X_test)\n",
        "  # Use accuracy_score function to get the accuracy\n",
        "  #print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
        "  #print(average_precision_score(predictions_SVM, Test_Y))\n",
        "  #print (classification_report(Test_Y, predictions_SVM))\n",
        "  return str(accuracy_score(predictions_SVM, Test_Y)*100)\n",
        "\n",
        "def G_NB(X_train,Train_Y,X_test,Test_Y):\n",
        "  #Create a Gaussian Classifier\n",
        "   model = GaussianNB()\n",
        "\n",
        "   # Train the model using the training sets\n",
        "   model.fit(X_train,Train_Y)\n",
        "\n",
        "   #Predict Output\n",
        "   predicted= model.predict(X_test) \n",
        "   #print(\"Accuracy:\",metrics.accuracy_score(Test_Y, predicted))\n",
        "   return str(metrics.accuracy_score(Test_Y, predicted))\n",
        "\n",
        "def KN_class(X_train,Train_Y,X_test,Test_Y):\n",
        "  classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "  classifier.fit(X_train,Train_Y)\n",
        "  y_pred = classifier.predict(X_test)\n",
        "  #print(\"Accuracy:\",metrics.accuracy_score(Test_Y, y_pred))\n",
        "  return str(metrics.accuracy_score(Test_Y, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfbCDS0e4kXm"
      },
      "source": [
        "def get_sum_embedding(sent,pos):\n",
        "  doc=nlp(sent)\n",
        "  list_str=[]\n",
        "  vv = np.zeros((1,50))\n",
        "  for k,s in enumerate(doc.sents):\n",
        "    for j,token in enumerate(s):\n",
        "      if (token.idx==pos):\n",
        "        print(s,len(s),token.idx, token.text)\n",
        "        for i in range(0,len(s)):\n",
        "          if str(s[i])==token.text or check_special(str(s[i]))==False:\n",
        "            continue\n",
        "          else:\n",
        "            #get embedding\n",
        "            vv=vv+get_embedding('/content/drive/My Drive/Colab Notebooks/glove.6B.50d.txt',str(s[i]).lower(), 50)\n",
        "            #print(s[i],get_embedding('/content/drive/My Drive/Colab Notebooks/glove.6B.50d.txt',str(s[i]).lower(), 50))\n",
        "  return vv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo9x2JVV8pi5"
      },
      "source": [
        "#Get Embedding Vector\n",
        "def get_embedding(filepath, word_, embedding_dim):\n",
        "  #vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "  v = np.zeros((1,50))\n",
        "  with open(filepath) as f:\n",
        "    for line in f:\n",
        "      word, *vector = line.split()\n",
        "      if word ==word_:\n",
        "         v = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "  return v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoJBGVSVFPiR"
      },
      "source": [
        "#ignore special sympols\n",
        "def check_special(wordSpecial):\n",
        "  t= False\n",
        "  if(bool(re.search('^[a-zA-Z0-9]*$',wordSpecial))==True):\n",
        "    t=True\n",
        "  return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNCiKNJYgAA0"
      },
      "source": [
        "#customize sentence boundaries\n",
        "import spacy\n",
        "text = \"Past hospitalizations occurred in _%#MM2001#%_ with dehydration and during that hospitalization was found to have increase in the very long chain fatty acid level and therefore diagnosed with ALD. He also has had bronchitis approximately yearly but is on no medications and has not been diagnosed with asthma. ADMISSION MEDICATIONS: 1. Cortef 5 mg p.o. b.i.d. 2. Florinef 0.1 mg p.o. q.d. SOCIAL HISTORY: He lives in Michigan with his mother, mother's roommate, maternal grandmother, and brother on a farm.\"\n",
        "#text='His current history is precisely the same as it always has been at _%#CITY#%_ Childrens as well as his prior admission when he left AMA approximately 24 hours ago. There are no changes. For past medical history, family history, social history, allergies, and current medications, please see my dictation dated _%#MM#%_ _%#DD#%_, 2005, in FCIS.'\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "print(\"Before:\", [sent.text for sent in doc.sents])\n",
        "\n",
        "def set_custom_boundaries(doc):\n",
        "    for token in doc[:-1]:\n",
        "        if token.text=='%' or token.text.endswith('%'):\n",
        "            doc[token.i+1].is_sent_start = False\n",
        "    return doc\n",
        "\n",
        "nlp.add_pipe(set_custom_boundaries, before=\"parser\")\n",
        "doc = nlp(text)\n",
        "print(\"After:\", [sent.text for sent in doc.sents])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b87p1mC-dD_G"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwPN6DEurKua"
      },
      "source": [
        "#under developing to check sentence boundaries\n",
        "for token in doc:\n",
        " print(token.is_sent_start, ' '+token.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeOdoRn4Ls3N"
      },
      "source": [
        "sense= []\n",
        "embedding_matrix = np.zeros((1, 50))\n",
        "\n",
        "for index,row in sub_pd.iterrows():\n",
        "  L_Side,R_Side=get_sides(str(row[6]))\n",
        "  embedding_matrix = np.vstack([embedding_matrix, get_embedding_feature(L_Side,R_Side)])\n",
        "\n",
        "x = np.delete(embedding_matrix, (0), axis=0)\n",
        "\n",
        "for index, row in sub_pd.iterrows():\n",
        "  sense.append(str(row[1]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2gOPSjy14Sm"
      },
      "source": [
        "#prepare data for the model and train the model\n",
        "\n",
        "X_train, X_test, Train_Y, Test_Y= train_test_data(x,sense)\n",
        "\n",
        "print(SVM_class(X_train, X_test, Train_Y, Test_Y))\n",
        "print(G_NB(X_train,Train_Y,X_test,Test_Y))\n",
        "print(KN_class(X_train,Train_Y,X_test,Test_Y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLcS8wserwc-"
      },
      "source": [
        "#get left and right side\n",
        "def get_sides(sent):\n",
        "  doc=nlp(sent)\n",
        "  for k,s in enumerate(doc.sents):\n",
        "    for j,token in enumerate(s):\n",
        "      if (token.idx==132):\n",
        "        if j !=0:\n",
        "          L_Side=s[0:j-1]\n",
        "        else:\n",
        "          L_Side=[]\n",
        "        if j != len(s):\n",
        "          R_Side=s[j+1:len(s)-1]\n",
        "        else:\n",
        "          R_Side=[]\n",
        "  return L_Side,R_Side"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8eeuIbeY0dX"
      },
      "source": [
        "#ignore special sympols\n",
        "def check_special(wordSpecial):\n",
        "  t= False\n",
        "  if (bool(re.search('^[a-zA-Z0-9]*$',wordSpecial))==True) and wordSpecial.isdigit()==False:\n",
        "      t=True\n",
        "  return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9kteK-rk30c"
      },
      "source": [
        "#get_feature\n",
        "def get_embedding_feature(l,r):\n",
        "  l,r=get_sides(text)\n",
        "  window_size=r_window_size=0\n",
        "  for i in range(len(l) ,0 , -1):\n",
        "    if check_special(str(l[i])) and window_size < 5:\n",
        "      l_em=get_embedding('/content/drive/My Drive/Colab Notebooks/glove.6B.50d.txt', str(l[i]).lower(), 50)\n",
        "      window_size=window_size+1\n",
        "\n",
        "  for j in range(0,len(r) ):\n",
        "    if check_special(str(r[j])) and r_window_size < 5: \n",
        "      r_em=get_embedding('/content/drive/My Drive/Colab Notebooks/glove.6B.50d.txt', str(r[j]).lower(), 50)\n",
        "      r_window_size = r_window_size+1\n",
        "\n",
        "  total_em=l_em+r_em/(window_size+r_window_size)\n",
        "  return total_em\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwiq8hqvRQPC"
      },
      "source": [
        "CORRECT  some annotations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaRHdtTGRn1R"
      },
      "source": [
        "#customize sentence boundaries\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "#doc = nlp(text)\n",
        "#print(\"Before:\", [sent.text for sent in doc.sents])\n",
        "\n",
        "def set_custom_boundaries(doc):\n",
        "    for token in doc[:-1]:\n",
        "        if token.text=='%' or token.text.endswith('%'):\n",
        "            doc[token.i+1].is_sent_start = False\n",
        "    return doc\n",
        "\n",
        "nlp.add_pipe(set_custom_boundaries, before=\"parser\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytFLikZzRPSW"
      },
      "source": [
        "import pandas as pd\n",
        "txt_path='/content/drive/My Drive/Colab Notebooks/AnonymizedClinicalAbbreviationsAndAcronymsDataSet.txt'\n",
        "data_pd=pd.read_csv( txt_path, sep=\"|\", header=None,encoding='cp1252',)\n",
        "# detect annotation error\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTx-j_YiV1R5",
        "outputId": "130cd57b-9955-4c94-954b-d4cb60d0615b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#BAL', 'BK', 'C3', 'CEA', 'CVA', 'CVP', 'CVS', 'DIP', 'ER', 'FISH','NAD', 'OTC', 'PE', 'PT', 'SBP', 'SS', 'ASA', 'AMA'\n",
        "#sub_pd=data_pd.loc[data_pd[0]=='ASA']\n",
        "sub_pd.head()\n",
        "\n",
        "for index,row in sub_pd.iterrows():\n",
        "    doc=nlp(str(row[6]))\n",
        "    for k,s in enumerate(doc.sents):\n",
        "        \n",
        "        for j,token in enumerate(s):\n",
        "            \n",
        "            if(token.idx==row[3] and token.text != row[0]):\n",
        "                \n",
        "                print(index, token.idx,token.text,s[j+1],s[j+1].idx)\n",
        "                \n",
        "                # modify \n",
        "                sub_pd.at[index,3]=s[j+1].idx\n",
        "                "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2539 150 ( ASA 151\n",
            "2551 134 ( ASA 135\n",
            "2602 341 ( ASA 342\n",
            "2753 249 ( ASA 250\n",
            "2807 171 ( ASA 172\n",
            "2831 230 ( ASA 231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPcicFzf4hI1",
        "outputId": "205ec5f3-a3a1-44bc-e527-556a72ee91aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#sub_18_abb=pd.DataFrame()\n",
        "sub_18_abb=sub_18_abb.append(sub_pd)\n",
        "print(sub_18_abb.shape)\n",
        "print(sub_18_abb[0].unique())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8972, 7)\n",
            "['SS' 'SBP' 'PT' 'PE' 'OTC' 'NAD' 'FISH' 'ER' 'DIP' 'CVS' 'CVP' 'CVA'\n",
            " 'CEA' 'C3' 'BK' 'BAL' 'AMA' 'ASA']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8uU_c8KWDNu"
      },
      "source": [
        "sub_pd=sub_pd.drop(index=2916)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49xNFT7vVDDa"
      },
      "source": [
        "for index,row in sub_pd.iterrows():\n",
        "    doc=nlp(str(row[6]))\n",
        "    for k,s in enumerate(doc.sents):\n",
        "        \n",
        "        for j,token in enumerate(s):\n",
        "            \n",
        "            if(token.idx==row[3] and token.text != row[0]):\n",
        "                \n",
        "                print(index, token.idx,token.text,s[j+1],s[j+1].idx)\n",
        "                \n",
        "                # modify \n",
        "                #sub_pd.at[index,3]=s[j+1].idx\n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxi8BjmRVNQi"
      },
      "source": [
        "sub_18_abb=pd.DataFrame()\n",
        "sub_18_abb=sub_18_abb.append(sub_pd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ_t3DDKViXC",
        "outputId": "b741971a-d0e8-4233-bd11-6476f29d6ba2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sub_18_abb=sub_18_abb.append(sub_pd)\n",
        "sub_18_abb.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8972, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA3NLSoTcfVN",
        "outputId": "0d41b032-9aff-4953-d118-b06cab42dd07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sub_18_abb[0].unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['AMA', 'ASA', 'SS'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzmuvQwMglR3"
      },
      "source": [
        "sub_18_abb.to_csv('sub_18_abb_modified.tsv',sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzEALgHeuX0l"
      },
      "source": [
        ""
      ]
    }
  ]
}